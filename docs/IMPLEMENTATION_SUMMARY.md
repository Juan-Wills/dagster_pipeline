# Dagster Sensor Implementation Summary

## âœ… What Was Created

### 1. **Google Drive Resource** 
   - **File**: `pipeline/resources/google_drive_resource.py`
   - **Purpose**: Wraps Google Drive API for Dagster
   - **Key Features**:
     - Authentication handling
     - Folder search and creation
     - File upload/download
     - File listing with metadata

### 2. **ETL Asset**
   - **File**: `pipeline/assets/google_drive_etl.py`
   - **Asset Name**: `process_drive_files`
   - **Purpose**: Downloads files from raw_data, processes them, uploads to processed_data
   - **Current Processing**: Pass-through (copy files as-is)
   - **Ready for**: Adding pandas, transformations, validation, etc.

### 3. **File Monitor Sensor**
   - **File**: `pipeline/sensors/sensors.py`
   - **Sensor Name**: `google_drive_new_file_sensor`
   - **Purpose**: Monitors Google Drive for new CSV files
   - **Trigger**: Runs asset when new files detected
   - **Frequency**: Every 60 seconds
   - **State Management**: Uses cursor to track seen files

### 4. **Configuration**
   - **File**: `pipeline/definitions.py`
   - **Combines**: Assets, Resources, and Sensors
   - **Configures**: Google Drive credentials path

## ğŸ”„ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Google Drive "raw_data" Folder                             â”‚
â”‚  ğŸ“ Contains: employee.csv, sales.csv, etc.                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ” SENSOR: google_drive_new_file_sensor                    â”‚
â”‚  â€¢ Checks every 60 seconds                                  â”‚
â”‚  â€¢ Lists all CSV files in raw_data                          â”‚
â”‚  â€¢ Compares with cursor (previously seen files)             â”‚
â”‚  â€¢ Detects: new_files = current_files - seen_files          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ New files detected?
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš¡ TRIGGER: RunRequest                                     â”‚
â”‚  â€¢ Creates run with tags (file names, count)                â”‚
â”‚  â€¢ Updates cursor with new file IDs                         â”‚
â”‚  â€¢ Triggers asset materialization                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ­ ASSET: process_drive_files                              â”‚
â”‚                                                               â”‚
â”‚  Step 1: Download                                            â”‚
â”‚    â€¢ Connects to Google Drive                                â”‚
â”‚    â€¢ Downloads all CSV files from raw_data                   â”‚
â”‚    â€¢ Saves to: downloaded_csv/                               â”‚
â”‚                                                               â”‚
â”‚  Step 2: Process (Currently: Pass-through)                   â”‚
â”‚    â€¢ Reads file from downloaded_csv/                         â”‚
â”‚    â€¢ [Your processing logic here]                            â”‚
â”‚    â€¢ Writes to: processed_data/                              â”‚
â”‚                                                               â”‚
â”‚  Step 3: Upload                                              â”‚
â”‚    â€¢ Creates/finds processed_data folder in Drive            â”‚
â”‚    â€¢ Uploads processed files to Drive                        â”‚
â”‚    â€¢ Returns metadata (file count, names)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Google Drive "processed_data" Folder                        â”‚
â”‚  ğŸ“ Contains: processed files                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Sensor State Management

The sensor uses a **cursor** to track which files have been processed:

```json
{
  "file_ids": ["1abc", "2def", "3ghi"],
  "last_check": "2025-10-16T10:30:00"
}
```

- **First run**: Cursor is empty â†’ all files are "new"
- **Subsequent runs**: Only files not in cursor are "new"
- **After processing**: Cursor is updated with all current file IDs

## ğŸ¯ Key Features

### Idempotency
- Sensor tracks processed files via cursor
- Won't reprocess the same file unless cursor is reset
- Safe to run multiple times

### Monitoring
- Dagster UI shows sensor status
- Run history with logs
- Metadata on each run (file names, counts)

### Error Handling
- Skips execution if raw_data folder not found
- Logs errors for individual file uploads
- Continues processing even if one file fails

### Scalability
- Processes all new files in a single run
- Can handle multiple files detected simultaneously
- Configurable check interval

## ğŸ”§ Customization Points

### 1. Processing Logic
**Location**: `pipeline/assets/google_drive_etl.py` (line ~64)

Add your transformations:
```python
import pandas as pd

# Read CSV
df = pd.read_csv(local_path)

# Your transformations
df = df.dropna()
df['new_column'] = df['old_column'] * 2
df = df[df['value'] > 100]

# Save processed
df.to_csv(processed_path, index=False)
```

### 2. Sensor Frequency
**Location**: `pipeline/sensors/sensors.py` (line ~14)

```python
@sensor(
    minimum_interval_seconds=300,  # 5 minutes
    ...
)
```

### 3. File Types
**Location**: `pipeline/sensors/sensors.py` (line ~36)

```python
# Change from CSV to JSON
files = google_drive.list_files_in_folder(
    raw_folder_id, 
    mime_type='application/json'
)
```

### 4. Folder Names
**Location**: `pipeline/sensors/sensors.py` and `pipeline/assets/google_drive_etl.py`

Change `"raw_data"` and `"processed_data"` to your folder names.

## ğŸ“¦ Dependencies

```toml
[dependency-groups]
dagster = [
    "dagster>=1.11.14",
]
google-api = [
    "google-api-python-client>=2.184.0",
    "google-auth-httplib2>=0.2.0",
    "google-auth-oauthlib>=1.2.2",
]
```

## ğŸš€ Getting Started

1. **Install dependencies:**
   ```bash
   pip install -e ".[dagster,google-api]"
   ```

2. **Start Dagster:**
   ```bash
   dagster dev -f pipeline/definitions.py
   ```

3. **Enable sensor in UI:**
   - Navigate to http://localhost:3000
   - Go to Sensors tab
   - Enable `google_drive_new_file_sensor`

4. **Test:**
   - Upload CSV to Google Drive "raw_data"
   - Wait 60 seconds
   - Check Runs tab for execution

## ğŸ“ Next Steps

1. **Add processing logic** in the asset
2. **Add data validation** (e.g., schema checks)
3. **Add notifications** (Slack, email) on success/failure
4. **Add more sensors** for different folders or file types
5. **Add schedules** for regular processing
6. **Connect to database** for storing metadata
7. **Add tests** for your processing logic

## ğŸ› Common Issues

1. **ModuleNotFoundError**: Install dependencies with pip
2. **Authentication failed**: Delete `token.json` and re-authenticate
3. **Folder not found**: Create "raw_data" folder in Google Drive
4. **Sensor not triggering**: Make sure it's enabled in Dagster UI
5. **Files not detected**: Check they're CSV files in the root of raw_data
