# Dagster Pipeline

A production-ready Dagster ETL pipeline for processing CSV files from Google Drive using Docker.

## ğŸ—ï¸ Project Structure

```
dagster_pipeline/
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ dagster.yaml            # Dagster instance config
â”‚   â”œâ”€â”€ workspace.yaml          # Workspace definitions
â”‚   â””â”€â”€ __init__.py             # Settings and constants
â”‚
â”œâ”€â”€ dagster_pipeline/           # Main package
â”‚   â”œâ”€â”€ assets/                 # Data assets
â”‚   â”œâ”€â”€ resources/              # External resources (Google Drive, DB)
â”‚   â”œâ”€â”€ jobs/                   # Job definitions
â”‚   â”œâ”€â”€ sensors/                # Event sensors
â”‚   â”œâ”€â”€ schedules/              # Scheduled runs
â”‚   â””â”€â”€ definitions.py          # Central Dagster definitions
â”‚
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ raw/                    # Downloaded raw data
â”‚   â”œâ”€â”€ processed/              # Processed data
â”‚   â””â”€â”€ temp/                   # Temporary files
â”‚
â”œâ”€â”€ auth/                       # Authentication credentials
â”‚   â”œâ”€â”€ credentials.json        # Google API credentials
â”‚   â””â”€â”€ token.json             # OAuth token
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ setup_check.py         # Setup verification
â”‚   â””â”€â”€ quickstart.py          # Quick start guide
â”‚
â”œâ”€â”€ tests/                      # Test files
â”‚   â”œâ”€â”€ conftest.py            # Pytest configuration
â”‚   â””â”€â”€ test_*.py              # Test modules
â”‚
â”œâ”€â”€ docker/                     # Docker configuration
â”‚   â”œâ”€â”€ codespace.Dockerfile   # User code container
â”‚   â””â”€â”€ dagster_services.Dockerfile  # Dagster services
â”‚
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md
â”‚   â”œâ”€â”€ QUICKSTART_GUIDE.md
â”‚   â””â”€â”€ SENSOR_SETUP.md
â”‚
â”œâ”€â”€ docker-compose.yml          # Docker orchestration
â”œâ”€â”€ pyproject.toml             # Python dependencies
â”œâ”€â”€ setup.py                   # Package setup
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- Python 3.13+
- Google Cloud account with Drive API enabled
- `uv` package manager (optional, for local development)

### 1. Setup Google Drive Credentials

1. Create a project in [Google Cloud Console](https://console.cloud.google.com/)
2. Enable Google Drive API
3. Create OAuth 2.0 credentials
4. Download and save as `auth/credentials.json`

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env with your database credentials
```

### 3. Start the Pipeline

```bash
# Start all services
docker compose up -d

# Check service status
docker compose ps

# View logs
docker compose logs -f
```

### 4. Access Dagster UI

Open your browser to: http://localhost:3000

## ğŸ“Š Features

- **Google Drive Integration**: Automatically download and process CSV files
- **Sensors**: Detect new files and trigger processing
- **Docker-based**: Fully containerized for easy deployment
- **PostgreSQL Storage**: Persistent run history and metadata
- **Modular Design**: Easy to extend with new assets and resources

## ğŸ”§ Development

### Run Setup Check

```bash
python scripts/setup_check.py
```

### Local Development (without Docker)

```bash
# Install dependencies
uv sync --group dagster --group google-api

# Run Dagster dev server
dagster dev -f dagster_pipeline/definitions.py
```

### Run Tests

```bash
pytest tests/
```

## ğŸ“ Configuration

All configuration is centralized in `config/__init__.py`:

- Project paths
- Data directories
- Authentication paths

## ğŸ” Authentication

The first time you run the pipeline, it will:
1. Open a browser for Google OAuth
2. Request Drive API permissions
3. Save the token to `auth/token.json`

## ğŸ“š Documentation

- [Implementation Summary](docs/IMPLEMENTATION_SUMMARY.md)
- [Quick Start Guide](docs/QUICKSTART_GUIDE.md)
- [Sensor Setup](docs/SENSOR_SETUP.md)

## ğŸ› ï¸ Architecture

### Services

- **dagster-webserver**: Web UI (port 3000)
- **dagster-daemon**: Background scheduler and sensor runner
- **codespace**: User code server (port 4000)
- **postgres**: Metadata and run storage (port 5432)

### Data Flow

1. Sensor detects new CSV in Google Drive `raw_data` folder
2. Asset downloads file to `data/raw/`
3. Pandas processes the data (clean, transform)
4. Processed file saved to `data/processed/`
5. File uploaded to Google Drive `processed_data` folder

## ğŸ“¦ Package Management

The project uses `uv` for fast dependency management:

```bash
# Add a new dependency
uv add <package-name>

# Update dependencies
uv sync

# Lock dependencies
uv lock
```

## ğŸ› Troubleshooting

### Container Issues

```bash
# Rebuild containers
docker compose down
docker compose build --no-cache
docker compose up -d
```

### Check Logs

```bash
# All services
docker compose logs

# Specific service
docker compose logs codespace
```

### Reset Database

```bash
docker compose down -v
docker compose up -d
```

## ğŸ“„ License

MIT License

## ğŸ‘¤ Author

Juan Wills

---

For more details, see the [documentation](docs/).
